% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{dor}
\alias{dor}
\title{Diagnostic Odds Ratio (DOR)}
\usage{
# diagnostic odds ratio
dor(
  actual,
  predicted,
  aggregate = FALSE
)
}
\arguments{
\item{actual}{A <\link{factor}>-vector of \link{length} \eqn{n}, and \eqn{k} levels.}

\item{predicted}{A <\link{factor}>-vector of \link{length} \eqn{n}, and \eqn{k} levels.}

\item{aggregate}{A <\link{logical}>-value of \link{length} \eqn{1}. \link{FALSE} by default. If \link{TRUE} it returns the
micro average across all \eqn{k} classes}
}
\value{
If \code{aggregate} is \link{FALSE} (the default), a named <\link{numeric}>-vector of \link{length} k

If \code{aggregate} is \link{TRUE}, a <\link{numeric}>-vector of \link{length} 1
}
\description{
Placeholder
}
\details{
The Diagnostic Odds Ratio (DOR) is calculated for each class \eqn{k} as follows,

\deqn{
  \frac{\text{PLR}_k}{\text{NLR}_k} = \frac{\text{Sensitivity}_k \times \text{Specificity}_k}{(1 - \text{Sensitivity}_k) \times (1 - \text{Specificity}_k)}
}

Where sensitivity (or true positive rate) is calculated as \eqn{\frac{\#TP_k}{\#TP_k + \#FN_k}} and specificity (or true negative rate) is calculated as \eqn{\frac{\#TN_k}{\#TN_k + \#FP_k}}.

When \code{aggregate = TRUE}, the \code{micro}-average is calculated,

\deqn{
  \frac{\sum_{k=1}^k \text{PLR}_k}{\sum_{k=1}^k \text{NLR}_k} = \frac{\sum_{k=1}^k (\text{Sensitivity}_k \times \text{Specificity}_k)}{\sum_{k=1}^k (1 - \text{Sensitivity}_k) \times (1 - \text{Specificity}_k)}
}
}
\examples{
# 1) assume that actual
# and predicted are class labels
# from some model
actual <- sample(
  x = letters[1:3],
  size = 100,
  replace = TRUE,
  prob = c(0.2,0.2,0.6)
)

predicted <- sample(
  x = letters[1:3],
  size = 100,
  replace = TRUE,
  prob = c(0.2,0.2,0.6)
)

# 1.1) convert to factor
# variable
actual <- factor(
  x = actual,
  levels = letters[1:3]
)

predicted <- factor(
  x = predicted,
  levels = letters[1:3]
)


# 2) evaluate the
# performance of all classes
specificity(
  actual    = actual,
  predicted = predicted
)

# 3) evaluate the
# overall performance
# with micro averaging
specificity(
  actual    = actual,
  predicted = predicted,
  aggregate = TRUE
)

# 4) evaluate the
# overall performance
# with macro averaging
mean(
  specificity(
    actual    = actual,
    predicted = predicted,
    aggregate = FALSE
  )
)
}
\seealso{
Other classification: 
\code{\link{accuracy}()},
\code{\link{cmatrix}()},
\code{\link{fbeta}()},
\code{\link{fdr}()},
\code{\link{fer}()},
\code{\link{fmi}()},
\code{\link{fpr}()},
\code{\link{jaccard}()},
\code{\link{kappa}()},
\code{\link{mcc}()},
\code{\link{nlr}()},
\code{\link{npv}()},
\code{\link{plr}()},
\code{\link{precision}()},
\code{\link{recall}()},
\code{\link{specificity}()},
\code{\link{zerooneloss}()}
}
\concept{classification}
