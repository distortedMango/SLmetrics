% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/RcppExports.R
\name{kappa}
\alias{kappa}
\title{Cohens \eqn{\kappa}-statistic}
\usage{
kappa(
  actual,
  predicted,
  beta = 0
)
}
\arguments{
\item{actual}{A <\link{factor}>-vector of length n, and k levels.}

\item{predicted}{A <\link{factor}>-vector of length n.}

\item{beta}{A <\link{numeric}> value of \link{length} 1. 0 by default. If set to a value different from zero, the off-diagonal confusion matrix will be penalized.}
}
\description{
Cohens
}
\details{
This function calculates the penalized kappa statistic as:

\deqn{\kappa = 1 - \frac{n_{disagree}}{n_{chance}}}

where \eqn{n_{disagree}} is the weighted disagreement, and
\eqn{n_{chance}} is the expected disagreement by chance. The function
operates as follows:
\enumerate{
\item \strong{Confusion Matrix}: The confusion matrix \eqn{C} is calculated based
on the input vectors \code{actual} and \code{predicted}.
\item \strong{Penalizing Matrix}: A penalizing matrix \eqn{P} is created using
\code{seqmat}, based on the dimensions of the confusion matrix.
The penalizing matrix assigns weights to each element based on the
absolute differences between the row and column indices, raised to the power \eqn{\beta}.
When \eqn{\beta = 0}, the matrix elements corresponding to perfect agreement
(i.e., the diagonal elements) are set to 0, while all other entries are set to 1.
For higher values of \eqn{\beta}, the penalties grow as the absolute difference between the
indices increases, giving greater penalization to more severe misclassifications.
\item \strong{Weighted Disagreement}: The observed weighted disagreement \eqn{n_{disagree}}
is calculated as:
\deqn{n_{disagree} = \sum_{i,j} C(i,j) P(i,j)}
where \eqn{C(i,j)} is the confusion matrix entry at row \eqn{i}, column \eqn{j}, and
\eqn{P(i,j)} is the corresponding penalizing matrix entry.
\item \strong{Expected Agreement}: The expected agreement by chance \eqn{n_{chance}} is
calculated based on the marginal row and column sums:
\deqn{n_{chance} = \sum_{i,j} \frac{R_i C_j}{N} P(i,j)}
where \eqn{R_i} is the sum of row \eqn{i} of the confusion matrix, \eqn{C_j} is the
sum of column \eqn{j}, and \eqn{N} is the total number of observations.
}

The penalized kappa statistic then measures the agreement adjusted for chance,
while penalizing different types of misclassifications depending on their severity
(as controlled by \eqn{\beta}).
}
\seealso{
Other classification: 
\code{\link{accuracy}()},
\code{\link{cmatrix}()},
\code{\link{dor}()},
\code{\link{fbeta}()},
\code{\link{fdr}()},
\code{\link{fer}()},
\code{\link{fpr}()},
\code{\link{nlr}()},
\code{\link{npv}()},
\code{\link{plr}()},
\code{\link{precision}()},
\code{\link{recall}()},
\code{\link{specificity}()},
\code{\link{zerooneloss}()}
}
\concept{classification}
