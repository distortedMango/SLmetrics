# Generated by using Rcpp::compileAttributes() -> do not edit by hand
# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393

#' Fowlkes-Mallows Index (FMI)
#'
#' @description
#' Calculate the Fowlkes-Mallows Index (FMI)
#'
#' @usage
#' # fowlkes-mallows index
#' fmi(
#'   actual,
#'   predicted
#' )
#'
#' @inherit specificity
#'
#' @details
#'
#' The FMI Index is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \sqrt{\frac{\#TP_k}{\#TP_k + \#FP_k} \times \frac{\#TP_k}{\#TP_k + \#FN_k}}
#' }
#'
#' Where \eqn{\#TP_k}, \eqn{\#FP_k}, and \eqn{\#FN_k} represent the number of true positives, false positives, and false negatives for each class \eqn{k}, respectively.
#'
#'
#' @returns
#' A <[numeric]> vector of [length] 1
#'
#' @family classification
#'
#' @export
fmi <- function(actual, predicted) {
    .Call(`_SLmetrics_fmi`, actual, predicted)
}

#' Accuracy
#'
#' Calculate the proportion of correct predictions.
#'
#' @usage
#' accuracy(
#'   actual,
#'   predicted
#' )
#'
#' @inherit specificity
#'
#' @details
#'
#' Accuracy is a global metric that measures the proportion of correct predictions (both true positives and true negatives) out of all predictions, and is calculated as follows,
#'
#' \deqn{
#'   \frac{\#TP + \#TN}{\#TP + \#TN + \#FP + \#FN}
#' }
#'
#' Where \eqn{\#TP}, \eqn{\#TN}, \eqn{\#FP}, and \eqn{\#FN} represent the true positives, true negatives, false positives, and false negatives, respectively.
#'
#' Accuracy provides an overall performance measure of the model across all classes.
#'
#' @returns
#'
#' A <[numeric]>-vector of [length] 1
#'
#' @example man/examples/scr_accuracy.R
#'
#' @family classification
#'
#' @export
accuracy <- function(actual, predicted) {
    .Call(`_SLmetrics_accuracy`, actual, predicted)
}

#' Confusion Matrix
#'
#' @description
#'
#' The [cmatrix()]-function uses cross-classifying factors to build
#' a confusion matrix of the counts at each combination of the [factor] levels.
#' Each row of the [matrix] represents the actual [factor] levels, while each
#' column represents the predicted [factor] levels.
#'
#' @usage
#' cmatrix(
#'   actual,
#'   predicted
#' )
#'
#' @param actual A <[factor]>-vector of [length] \eqn{n}, and \eqn{k} levels.
#' @param predicted A <[factor]>-vector of [length] \eqn{n}, and \eqn{k} levels.
#'
#' @example man/examples/scr_confusionmatrix.R
#' @family classification
#'
#' @details
#'
#' If the function is correctly implemented the resulting
#' confusion matrix is given as,
#'
#' |            | A (Predicted)        | B (Predicted)   |
#' | ------------- |:-------------:| -----:|
#' | A (Actual)   | Value     | Value |
#' | B  (Actual)   |  Value    |  Value   |
#'
#'
#' @returns A named \eqn{k} x \eqn{k} <[matrix]>
#'
#' @export
cmatrix <- function(actual, predicted) {
    .Call(`_SLmetrics_cmatrix`, actual, predicted)
}

#' Diagnostic Odds Ratio (DOR)
#'
#' @description
#' Placeholder
#'
#' @usage
#' # diagnostic odds ratio
#' dor(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @details
#'
#' The Diagnostic Odds Ratio (DOR) is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\text{PLR}_k}{\text{NLR}_k} = \frac{\text{Sensitivity}_k \times \text{Specificity}_k}{(1 - \text{Sensitivity}_k) \times (1 - \text{Specificity}_k)}
#' }
#'
#' Where sensitivity (or true positive rate) is calculated as \eqn{\frac{\#TP_k}{\#TP_k + \#FN_k}} and specificity (or true negative rate) is calculated as \eqn{\frac{\#TN_k}{\#TN_k + \#FP_k}}.
#'
#' When `aggregate = TRUE`, the `micro`-average is calculated,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \text{PLR}_k}{\sum_{k=1}^k \text{NLR}_k} = \frac{\sum_{k=1}^k (\text{Sensitivity}_k \times \text{Specificity}_k)}{\sum_{k=1}^k (1 - \text{Sensitivity}_k) \times (1 - \text{Specificity}_k)}
#' }
#'
#' @inherit specificity
#'
#' @family classification
#' @export
dor <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_dor`, actual, predicted, aggregate)
}

#' Generalized F Score
#'
#' @description
#' Calculate the F Score
#'
#' @usage
#' # fbeta-score
#' fbeta(
#'   actual,
#'   predicted,
#'   beta = 1,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @param beta A <[numeric]> vector of length 1. 1 by default, see details.
#' @param aggregate A <[logical]>-value of [length] 1. [FALSE] by default. If [TRUE] it returns the
#' micro average across all k-classes
#'
#' @details
#'
#'
#' The F-beta score is a weighted harmonic mean of precision and recall, calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   (1 + \beta^2) \cdot \frac{\text{Precision}_k \cdot \text{Recall}_k}{(\beta^2 \cdot \text{Precision}_k) + \text{Recall}_k}
#' }
#'
#' Where precision is \eqn{\frac{\#TP_k}{\#TP_k + \#FP_k}} and recall (sensitivity) is \eqn{\frac{\#TP_k}{\#TP_k + \#FN_k}}, and \eqn{\beta} determines the weight of precision relative to recall.
#'
#' When `aggregate = TRUE`, the `micro`-average F-beta score is calculated,
#'
#' \deqn{
#'   (1 + \beta^2) \cdot \frac{\sum_{k=1}^K \text{Precision}_k \cdot \sum_{k=1}^K \text{Recall}_k}{(\beta^2 \cdot \sum_{k=1}^K \text{Precision}_k) + \sum_{k=1}^K \text{Recall}_k}
#' }
#'
#'
#' @family classification
#'
fbeta <- function(actual, predicted, beta = 1.0, aggregate = FALSE) {
    .Call(`_SLmetrics_fbeta`, actual, predicted, beta, aggregate)
}

#' False Discovery Rate (FDR)
#'
#' @description
#' Placeholder
#'
#' @usage
#' # false discovery rate;
#' fdr(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @details
#'
#' The False Discovery Rate (FDR). The metric is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#FP_k}{\#TP_k+\#FP_k}
#' }
#'
#' Where \eqn{\#TP_k} and \eqn{\#FP_k} is the number of true psotives and false positives, respectively, for each class \eqn{k}.
#'
#' When `aggregate = TRUE` the `micro`-average is calculated,
#'
#' \deqn{
#'  \frac{\sum_{k=1}^k \#FP_k}{\sum_{k=1}^k \#TP_k + \sum_{k=1}^k \#FP_k}
#' }
#'
#' @family classification
#' @export
fdr <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_fdr`, actual, predicted, aggregate)
}

#' False Exclusion Rate (FER)
#'
#' The [fer()]-function calculates the False Omission Rate (FOR)
#'
#'
#' @usage
#' # false omission rate
#' fer(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @details
#'
#' The False Omission Rate (FOR) is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#FN_k}{\#FN_k + \#TN_k}
#' }
#'
#' Where \eqn{\#FN_k} and \eqn{\#TN_k} are the number of false negatives and true negatives, respectively, for each class \eqn{k}.
#'
#' When `aggregate = TRUE`, the `micro`-average is calculated,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \#FN_k}{\sum_{k=1}^k \#FN_k + \sum_{k=1}^k \#TN_k}
#' }
#'
#' @family classification
#' @export
fer <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_fer`, actual, predicted, aggregate)
}

#' False Positive Rate (FPR)
#'
#' @description
#' Calculate the False Positive Rate (FPR), also known as the fall-out rate ([fallout()]), which represents the proportion of negative instances that were incorrectly classified as positive.
#'
#' @usage
#' # using`fpr()`
#' fpr(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#'
#' @details
#'
#' The False Positive Rate (FPR) for each class \eqn{k} is calculated as follows,
#'
#' \deqn{
#'   \frac{\#FP_k}{\#FP_k + \#TN_k}
#' }
#'
#' Where \eqn{\#FP_k} and \eqn{\#TN_k} represent the number of false positives and true negatives, respectively, for each class \eqn{k}.
#'
#' When `aggregate = TRUE`, the micro-average is calculated across all classes,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \#FP_k}{\sum_{k=1}^k \#FP_k + \sum_{k=1}^k \#TN_k}
#' }
#'
#' The FPR is the complement of specificity, such that \eqn{\text{FPR} = 1 - \text{Specificity}}.
#'
#' @family classification
#' @export
fpr <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_fpr`, actual, predicted, aggregate)
}

#' @rdname fpr
#' @usage
#' # using `fallout()`
#' fallout(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#' @export
fallout <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_fallout`, actual, predicted, aggregate)
}

#' Jaccard Index
#'
#' @description
#' The Jaccard Index measures similarity between finite sample sets and is defined as the size of the intersection divided by the size of the union of the sample sets.
#'
#' @usage
#' # using `jaccard()`-function
#' jaccard(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @details
#'
#' The Jaccard Index is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#TP_k}{\#TP_k + \#FP_k + \#FN_k}
#' }
#'
#' Where \eqn{\#TP_k}, \eqn{\#FP_k}, and \eqn{\#FN_k} represent the number of true positives, false positives, and false negatives for each class \eqn{k}, respectively.
#'
#' When `aggregate = TRUE`, the micro-average is calculated as,
#'
#' \deqn{
#'   \frac{\sum_{i = 1}^{k} TP_i}{\sum_{i = 1}^{k} TP_i + \sum_{i = 1}^{k} FP_k + \sum_{i = 1}^{k} FN_k}
#' }
#'
#' @example man/examples/scr_jaccard.R
#'
#' @family classification
#'
#' @export
jaccard <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_jaccard`, actual, predicted, aggregate)
}

#' @rdname jaccard
#'
#' @usage
#' # using `csi()`-function
#' csi(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#' @export
csi <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_csi`, actual, predicted, aggregate)
}

#' @rdname jaccard
#'
#' @usage
#' # using `tscore()`-function
#' tscore(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#' @export
tscore <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_tscore`, actual, predicted, aggregate)
}

#' Cohens \eqn{\kappa}-statistic
#'
#' @description
#'
#' Cohens
#'
#' @usage
#' kappa(
#'   actual,
#'   predicted,
#'   beta = 0
#' )
#'
#' @inheritParams cmatrix
#' @param beta A <[numeric]> value of [length] 1. 0 by default. If set to a value different from zero, the off-diagonal confusion matrix will be penalized.
#'
#'
#' @details
#' This function calculates the penalized kappa statistic as:
#'
#'   \deqn{\kappa = 1 - \frac{n_{disagree}}{n_{chance}}}
#'
#' where \eqn{n_{disagree}} is the weighted disagreement, and
#' \eqn{n_{chance}} is the expected disagreement by chance. The function
#' operates as follows:
#'
#' 1. **Confusion Matrix**: The confusion matrix \eqn{C} is calculated based
#'    on the input vectors \code{actual} and \code{predicted}.
#'
#' 2. **Penalizing Matrix**: A penalizing matrix \eqn{P} is created using
#'    \code{seqmat}, based on the dimensions of the confusion matrix.
#'    The penalizing matrix assigns weights to each element based on the
#'    absolute differences between the row and column indices, raised to the power \eqn{\beta}.
#'    When \eqn{\beta = 0}, the matrix elements corresponding to perfect agreement
#'    (i.e., the diagonal elements) are set to 0, while all other entries are set to 1.
#'    For higher values of \eqn{\beta}, the penalties grow as the absolute difference between the
#'    indices increases, giving greater penalization to more severe misclassifications.
#'
#' 3. **Weighted Disagreement**: The observed weighted disagreement \eqn{n_{disagree}}
#'    is calculated as:
#'    \deqn{n_{disagree} = \sum_{i,j} C(i,j) P(i,j)}
#'    where \eqn{C(i,j)} is the confusion matrix entry at row \eqn{i}, column \eqn{j}, and
#'    \eqn{P(i,j)} is the corresponding penalizing matrix entry.
#'
#' 4. **Expected Agreement**: The expected agreement by chance \eqn{n_{chance}} is
#'    calculated based on the marginal row and column sums:
#'    \deqn{n_{chance} = \sum_{i,j} \frac{R_i C_j}{N} P(i,j)}
#'    where \eqn{R_i} is the sum of row \eqn{i} of the confusion matrix, \eqn{C_j} is the
#'    sum of column \eqn{j}, and \eqn{N} is the total number of observations.
#'
#' The penalized kappa statistic then measures the agreement adjusted for chance,
#' while penalizing different types of misclassifications depending on their severity
#' (as controlled by \eqn{\beta}).
#'
#' @family classification
#'
kappa <- function(actual, predicted, beta = 0) {
    .Call(`_SLmetrics_kappa`, actual, predicted, beta)
}

#' Positive Likelihood (LR+)
#' @usage
#' plr(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @details
#'
#' The Positive Likelihood Ratio (PLR) is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\text{Sensitivity}_k}{1 - \text{Specificity}_k}
#' }
#'
#' Where sensitivity (or true positive rate) is calculated as \eqn{\frac{\#TP_k}{\#TP_k + \#FN_k}} and specificity (or true negative rate) is calculated as \eqn{\frac{\#TN_k}{\#TN_k + \#FP_k}}.
#'
#' When `aggregate = TRUE`, the `micro`-average is calculated,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \text{Sensitivity}_k}{1 - \sum_{k=1}^k \text{Specificity}_k}
#' }
#'
#' @seealso
#'
#' The [nlr()]-function for the Negative Likehood Ratio (LR-)
#'
#' @family classification
#' @export
plr <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_plr`, actual, predicted, aggregate)
}

#' Negative Likelihood Ratio (LR-)
#'
#'
#' @usage
#' nlr(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @details
#'
#' The Negative Likelihood Ratio (NLR) is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{1 - \text{Sensitivity}_k}{\text{Specificity}_k}
#' }
#'
#' Where sensitivity (or true positive rate) is calculated as \eqn{\frac{\#TP_k}{\#TP_k + \#FN_k}} and specificity (or true negative rate) is calculated as \eqn{\frac{\#TN_k}{\#TN_k + \#FP_k}}.
#'
#' When `aggregate = TRUE`, the `micro`-average is calculated,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k (1 - \text{Sensitivity}_k)}{\sum_{k=1}^k \text{Specificity}_k}
#' }
#'
#' @seealso
#'
#' The [plr()]-function for the Positive Likehood Ratio (LR+)
#' @family classification
#' @export
nlr <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_nlr`, actual, predicted, aggregate)
}

#' Matthews Correlation Coefficient (MCC)
#'
#' @description
#' Calculate the Matthews Correlation Coefficient (MCC)
#'
#' @usage
#' # 1) `mcc()`-function
#' mcc(
#'   actual,
#'   predicted
#' )
#'
#' @inherit precision
#'
#' @details
#'
#' The MCC is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#TP_k \times \#TN_k - \#FP_k \times \#FN_k}{\sqrt{(\#TP_k + \#FP_k)(\#TP_k + \#FN_k)(\#TN_k + \#FP_k)(\#TN_k + \#FN_k)}}
#' }
#'
#'
#' @returns
#' A named <[numeric]> vector of length k
#'
#' @family classification
#'
#' @export
mcc <- function(actual, predicted) {
    .Call(`_SLmetrics_mcc`, actual, predicted)
}

#' @rdname mcc
#'
#' @usage
#' # 2) `phi()`-function
#' phi(
#'   actual,
#'   predicted
#' )
#'
#' @export
phi <- function(actual, predicted) {
    .Call(`_SLmetrics_phi`, actual, predicted)
}

#' Negative Predictive Value (NPV)
#'
#' @description
#' Calculate the sensitivity
#'
#' @usage
#' npv(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @details
#'
#' The Negative Predictive Value (NPV) is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#TN_k}{\#TN_k + \#FN_k}
#' }
#'
#' Where \eqn{\#TN_k} and \eqn{\#FN_k} are the number of true negatives and false negatives, respectively, for each class \eqn{k}.
#'
#' When `aggregate = TRUE`, the `micro`-average is calculated,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \#TN_k}{\sum_{k=1}^k \#TN_k + \sum_{k=1}^k \#FN_k}
#' }
#'
#' @family classification
#'
#' @export
npv <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_npv`, actual, predicted, aggregate)
}

#' Precision (Positive Predictive Value)
#'
#'
#' @description
#' Calculate the Precision
#'
#' @usage
#' # 1) `precision()`-function
#' precision(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @details
#'
#' The Precision (Positive Predictive Value, PPV) is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#TP_k}{\#TP_k + \#FP_k}
#' }
#'
#' Where \eqn{\#TP_k} and \eqn{\#FP_k} are the number of true positives and false positives, respectively, for each class \eqn{k}.
#'
#' When `aggregate = TRUE`, the `micro`-average is calculated,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \#TP_k}{\sum_{k=1}^k \#TP_k + \sum_{k=1}^k \#FP_k}
#' }
#'
#' @example man/examples/scr_precision.R
#'
#' @returns
#' A named <[numeric]> vector of length k
#'
#'
#' @family classification
#'
#' @export
precision <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_precision`, actual, predicted, aggregate)
}

#' @rdname precision
#'
#'
#' @usage
#' # 2) `ppv()`-function
#' ppv(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
ppv <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_ppv`, actual, predicted, aggregate)
}

#' Recall (Sensitivity)
#'
#' @description
#' Calculate the sensitivity
#'
#' @usage
#'  # 1) `recall()`-function
#' recall(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inherit specificity
#'
#' @details
#'
#' The Sensitivity (SEN), also known as Recall or True Positive Rate (TPR). The metric is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#TP_k}{\#TP_k + \#FN_k}
#' }
#'
#' Where \eqn{\#TP_k} and \eqn{\#FN_k} is the number of true positives and false negatives, respectively, for each class \eqn{k}.
#'
#' When `aggregate = TRUE` the `micro`-average is calculated,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \#TP_k}{\sum_{k=1}^k \#TP_k + \sum_{k=1}^k \#FN_k}
#' }
#' @family classification
#'
#' @export
recall <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_recall`, actual, predicted, aggregate)
}

#' @rdname recall
#'
#' @usage
#' tpr(
#'   actual,
#'   predicted,
#'   aggregate
#' )
#'
#' @export
tpr <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_tpr`, actual, predicted, aggregate)
}

#' @rdname recall
#' @usage
#' # 2) `sensitivity()`-function
#' sensitivity(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#' @export
sensitivity <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_sensitivity`, actual, predicted, aggregate)
}

#' @rdname specificity
NULL

#' @rdname specificity
NULL

#' Specificity (True Negative Rate)
#'
#' @description
#' Calculate the specificity
#'
#' @usage
#' # using `specificity()`
#' specificity(
#'   actual,
#'   predicted,
#'   aggregate = FALSE
#' )
#'
#' @inheritParams cmatrix
#' @param aggregate A <[logical]>-value of [length] \eqn{1}. [FALSE] by default. If [TRUE] it returns the
#' micro average across all \eqn{k} classes
#'
#'
#' @details
#'
#' The metric is calculated for each class \eqn{k} as follows,
#'
#' \deqn{
#'   \frac{\#TN_k}{\#TN_k+\#FP_k}
#' }
#'
#' Where \eqn{\#TN_k} and \eqn{\#FP_k} is the number of true negatives and false positives, respectively, for each class \eqn{k}.
#'
#' When `aggregate = TRUE` the `micro`-average is calculated,
#'
#' \deqn{
#'   \frac{\sum_{k=1}^k \#TN_k}{\sum_{k=1}^k \#TN_k + \sum_{k=1}^k \#FP_k}
#' }
#'
#' @returns
#'
#' If `aggregate` is [FALSE] (the default), a named <[numeric]>-vector of [length] k
#'
#' If `aggregate` is [TRUE], a <[numeric]>-vector of [length] 1
#'
#' @example man/examples/scr_specificity.R
#'
#' @family classification
#'
#' @export
specificity <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_specificity`, actual, predicted, aggregate)
}

tnr <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_tnr`, actual, predicted, aggregate)
}

selectivity <- function(actual, predicted, aggregate = FALSE) {
    .Call(`_SLmetrics_selectivity`, actual, predicted, aggregate)
}

#' Zero One Loss
#'
#' @description
#' Placeholder
#'
#' @usage
#' zerooneloss(
#'   actual,
#'   predicted
#' )
#'
#' @inherit specificity
#' @family classification
#'
#' @details
#'
#' Zero-One Loss is a global metric that measures the proportion of incorrect predictions made by the model. It is calculated as follows,
#'
#' \deqn{
#'   \frac{\#FP + \#FN}{\#TP + \#TN + \#FP + \#FN}
#' }
#'
#' Where \eqn{\#TP}, \eqn{\#TN}, \eqn{\#FP}, and \eqn{\#FN} represent the true positives, true negatives, false positives, and false negatives, respectively.
#'
#' Zero-One Loss provides an overall measure of the model's prediction errors across all classes.
#' @returns
#'
#' A <[numeric]>-vector of [length] 1
#'
#' @example man/examples/scr_accuracy.R
#'
#' @family classification
#'
#' @export
zerooneloss <- function(actual, predicted) {
    .Call(`_SLmetrics_zerooneloss`, actual, predicted)
}

#' Huber Loss
#'
#' @description
#'
#' Calculate the Huber Loss. [whuberloss()] calculates the arithmetic weighted average, and [huberloss()] calculates the arithmetic average.
#'
#' @usage
#' # simple mean
#' huberloss(
#'   actual,
#'   predicted,
#'   delta = 1
#' )
#'
#' @param actual A <[numeric]>-vector of length N. The observed (continuos) response variable.
#' @param predicted A <[numeric]>-vector of length N. The estimated (continuos) response variable.
#' @param delta A <[numeric]>-vector of length 1. 1 by default. The threshold value for switch between functions (see details).
#'
#' @details
#'
#' The Huber Loss is calculated as,
#'
#' \deqn{
#'  \frac{1}{2} (y_i - \hat{y}_i)^2 ~for~ |y_i - \hat{y}_i| \leq \delta
#' }
#'
#' \deqn{
#'   \delta |y_i-\hat{y}_i|-\frac{1}{2} \delta^2 ~for~ |y_i - \hat{y}_i| > \delta
#' }
#'
#' for each \eqn{i},
#'
#'
#' @example man/examples/scr_huberloss.R
#'
#'
#' @family regression
#'
#' @returns A <[numeric]>-value of [length] 1.
#'
#' @export
huberloss <- function(actual, predicted, delta = 1) {
    .Call(`_SLmetrics_huberloss`, actual, predicted, delta)
}

#' @rdname huberloss
#' @usage
#' # weighted mean
#' whuberloss(
#'   actual,
#'   predicted,
#'   w,
#'   delta = 1
#' )
#' @param w A <[numeric]>-vector of [length] N. The weight assigned to each observation in the data. See [stats::weighted.mean()] for more details.
#' @export
whuberloss <- function(actual, predicted, w, delta = 1) {
    .Call(`_SLmetrics_whuberloss`, actual, predicted, w, delta)
}

#' Mean Absolute Error (MAE)
#'
#' Calculate the MAE using the [mae()]-function for the (arithmetic) simple mean, or [wmae()]-function for the (arithmetic) weighted mean.
#'
#' @usage
#' # MAE (Simple Mean)
#' mae(
#'   actual,
#'   predicted
#' )
#'
#'
#' @inherit huberloss
#'
#' @family regression
#'
#' @export
mae <- function(actual, predicted) {
    .Call(`_SLmetrics_mae`, actual, predicted)
}

#' @rdname mae
#'
#' @usage
#' # MAE (Weighted Mean)
#' wmae(
#'   actual,
#'   predicted,
#'   w
#' )
#' @export
wmae <- function(actual, predicted, w) {
    .Call(`_SLmetrics_wmae`, actual, predicted, w)
}

#' Mean Squared Error (MSE)
#'
#' Calculate the MSE using the [mse()]-function for the (arithmetic) simple mean, or [wmse()]-function for the (arithmetic) weighted mean.
#'
#' @usage
#' # simple mean
#' mse(
#'   actual,
#'   predicted
#' )
#'
#' @inherit huberloss
#'
#' @family regression
#'
#' @export
mse <- function(actual, predicted) {
    .Call(`_SLmetrics_mse`, actual, predicted)
}

#' @rdname mse
#'
#' @usage
#' # weighted mean
#' wmse(
#'   actual,
#'   predicted,
#'   w
#' )
#' @export
wmse <- function(actual, predicted, w) {
    .Call(`_SLmetrics_wmse`, actual, predicted, w)
}

#' Root Mean Squared Error (RMSE)
#'
#' Calculate the RMSE using the [rmse()]-function for the (arithmetic) simple mean, or [wrmse()]-function for the (arithmetic) weighted mean.
#'
#' @usage
#' # simple RMSE
#' rmse(
#'   actual,
#'   predicted
#' )
#'
#' @inherit huberloss
#'
#' @family regression
#'
#' @export
rmse <- function(actual, predicted) {
    .Call(`_SLmetrics_rmse`, actual, predicted)
}

#' @rdname rmse
#'
#' @usage
#' # weighted RMSE
#' wrmse(
#'   actual,
#'   predicted,
#'   w
#' )
#' @export
wrmse <- function(actual, predicted, w) {
    .Call(`_SLmetrics_wrmse`, actual, predicted, w)
}

#' Root Mean Squared Logarithmic Error (RMSLE)
#'
#' Calculate the RMSLE using the [rmsle()]-function for the (arithmetic) simple mean, or [wrmsle()]-function for the (arithmetic) weighted mean.
#'
#' @usage
#' # simple RMSLE
#' rmsle(
#'   actual,
#'   predicted
#' )
#'
#' @inherit huberloss
#'
#' @family regression
#'
#' @export
rmsle <- function(actual, predicted) {
    .Call(`_SLmetrics_rmsle`, actual, predicted)
}

#' @rdname rmsle
#'
#' @usage
#' # weighted RMSLE
#' wrmsle(
#'   actual,
#'   predicted,
#'   w
#' )
#'
#' @export
wrmsle <- function(actual, predicted, w) {
    .Call(`_SLmetrics_wrmsle`, actual, predicted, w)
}

#' \eqn{R^2}
#'
#' @description
#' Calculate the R squared of two <[numeric]> vectors.
#'
#' @usage
#' rsq(
#'   actual,
#'   predicted,
#'   k = 0
#' )
#' @inherit huberloss
#' @param k A <[numeric]>-vector of length 1. 0 by default. If k>0
#' the function returns the adjusted R squared.
#'
#' @details
#'
#' The \eqn{R^2} is calculated as,
#'
#' \deqn{
#'   1 - \frac{SSE}{SST} \frac{n-1}{n - (k + 1)}
#' }
#'
#' @family regression
#'
#' @returns A <[numeric]>-value of length 1.
rsq <- function(actual, predicted, k = 0) {
    .Call(`_SLmetrics_rsq`, actual, predicted, k)
}

